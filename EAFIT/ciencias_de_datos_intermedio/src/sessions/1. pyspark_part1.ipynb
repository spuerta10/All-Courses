{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark #el encuentra spark\n",
    "findspark.init() #buscamos spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, FloatType #construir schema\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F #funciones para castear datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Â¿Para que es Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulacion de datos a escala de big data, procesamiento de grandes volumenes de datos.\n",
    "Surge de la programacion distribuida.\n",
    "MapReduce (Google) evoluciona a Hadoop (capturar varias maquinas para que funcionen como una) y luego ApacheSpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ApacheSpark (forma local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ApacheSpark (forma local):\n",
    "Particiona la CPU para ejecutar de forma distribuida el procesamiento.\n",
    "Es ideal solo para la manipulacion de dataframes.\n",
    "Paradigma orientada a eventos (tiene que pasar algo para ejecutar un codigo).\n",
    "El driver (Linea de codigo de Spark) se encarga de controlar todo\n",
    "El cluster manager se encarga de controlar que los ejecutores cumplan con su tarea.\n",
    "Cada ejecutor tiene diferentes cosas que ejecutar (tareas).\n",
    "Los ejecutores se alojan en un nodo, que es cuanta CPU y RAM va a tener el ejecutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMaster\\nlocal[*] Esta usando todos los nucleos del procesador\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate() #obtener de la cesion el contexto y crear\n",
    "\"\"\"\n",
    "Master\n",
    "local[*] Esta usando todos los nucleos del procesador\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "spark = SparkSession.builder.appName(\"PySparkSession\") #cambiar el nombre de la app\n",
    ".confg(\"spark.master\", \"local[5]\") #colocar numero de nucleos\n",
    ".confg(\"spark.shuffle.sql.partitions\", \"3\") #numero de particiones\n",
    ".getOrCreate() \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkSession\")\\\n",
    "    .config(\"spark.master\", \"local[5]\")\\\n",
    "    .config(\"spark.shuffle.sql.partitions\", \"3\")\\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.master', 'local[6]') #cambiar numero de nucleos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero definir el schema: nombre de las columnas y los datos\n",
    "columns = [\"id\", \"name\", \"subject\", \"grade\"]\n",
    "values = [\n",
    "    (1, \"alumno_1\", \"calculo\",  4.2),\n",
    "    (2, \"alumno_2\", \"fisica\",   4.2),\n",
    "    (3, \"alumno_3\", \"etica\",    4.2),\n",
    "    (4, \"alumno_4\", \"geometria\",4.2),\n",
    "]\n",
    "\n",
    "#esto no saca de por si un dataframe, saca un dataframe particionado\n",
    "df = spark.createDataFrame(data = values, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\":1, \"nombre\":\"alumno_1\", \"asignatura\":\"calculo\",  \"nota\":4.2},\n",
    "    {\"id\":2, \"nombre\":\"alumno_2\", \"asignatura\":\"fisica\",   \"nota\":4.2},\n",
    "    {\"id\":3, \"nombre\":\"alumno_3\", \"asignatura\":\"etica\",    \"nota\":4.2},\n",
    "    {\"id\":4, \"nombre\":\"alumno_4\", \"asignatura\":\"geometria\",\"nota\":4.2},\n",
    "]\n",
    "df_dict = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = spark.createDataFrame(\n",
    "    pd.DataFrame(columns = columns, data = values)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+-----+\n",
      "| id|    name|  subject|grade|\n",
      "+---+--------+---------+-----+\n",
      "|  1|alumno_1|  calculo|  4.2|\n",
      "|  2|alumno_2|   fisica|  4.2|\n",
      "|  3|alumno_3|    etica|  4.2|\n",
      "|  4|alumno_4|geometria|  4.2|\n",
      "+---+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #mostrar dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'subject', 'grade']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #cuantas filas tiene\n",
    "len(df.columns) #cuantas columnas tiene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones y acciones:\n",
    "\n",
    "Transformaciones : Transforma la data (veloces)\n",
    "\n",
    "Accion: Aquel que obtiene o deja tangible la transformacion (no es veloz)\n",
    "\n",
    "Hacer la mayor cantidad de transformaciones y la menor cantidad de acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#acciones\n",
    "#para sacar una fila en especifico sacar todas las del df y filtrar por la fila en especifico\n",
    "df.take(1) #cuantas filas quiero obtener\n",
    "df.take(1)[0].id #acceder al id de la fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformaciones\n",
    "df_expr = df.selectExpr(\"grade + 0.5 as new_grade\") #aplicar expresion a una columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes siempre tienen esquema: Nombre de la columna, tipo de dato y si permite nulos.\n",
    "\n",
    "Una buena practica es que siempre al leer un archivo con PySpark crear el esquema del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- grade: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"nombre\", StringType(), False),\n",
    "        StructField(\"asignatura\", StringType(), False),\n",
    "        StructField(\"nota\", FloatType(), False)\n",
    "    ]\n",
    ") \n",
    "\n",
    "data = [\n",
    "    {\"id\":1, \"nombre\":\"alumno_1\", \"asignatura\":\"calculo\",  \"nota\":4.2},\n",
    "    {\"id\":2, \"nombre\":\"alumno_2\", \"asignatura\":\"fisica\",   \"nota\":4.2},\n",
    "    {\"id\":3, \"nombre\":\"alumno_3\", \"asignatura\":\"etica\",    \"nota\":4.2},\n",
    "    {\"id\":4, \"nombre\":\"alumno_4\", \"asignatura\":\"geometria\",\"nota\":4.2},\n",
    "]\n",
    "\n",
    "df_notas = spark.createDataFrame(data = data, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#acceder a columna de Spark\n",
    "df_notas[[\"id\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|nota|\n",
      "+---+----+\n",
      "|  1| 4.2|\n",
      "|  2| 4.2|\n",
      "|  3| 4.2|\n",
      "|  4| 4.2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#acceder a multiples columnas con Spark\n",
    "df_notas.select([\"id\", \"nota\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+\n",
      "| id|  nombre|asignatura|grade|\n",
      "+---+--------+----------+-----+\n",
      "|  1|alumno_1|   calculo|  4.2|\n",
      "|  2|alumno_2|    fisica|  4.2|\n",
      "|  3|alumno_3|     etica|  4.2|\n",
      "|  4|alumno_4| geometria|  4.2|\n",
      "+---+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renombrando columnas\n",
    "df_notas = df_notas.withColumnRenamed(\"nota\", \"grade\")\\\n",
    "                   .withColumnRenamed(\"nombre\", \"name\")\n",
    "df_notas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#castear variables Spark\n",
    "df_notas = df_notas.withColumn(\"grade\", F.col(\"grade\").cast(IntegerType())) #F.col para acceder directamente a la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+\n",
      "| id|  nombre|asignatura|grade|\n",
      "+---+--------+----------+-----+\n",
      "|  4|alumno_4| geometria|    4|\n",
      "|  3|alumno_3|     etica|    4|\n",
      "|  2|alumno_2|    fisica|    4|\n",
      "|  1|alumno_1|   calculo|    4|\n",
      "+---+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ordenar datos descendente\n",
    "df_notas.sort(\n",
    "    F.desc(\"id\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+\n",
      "| id|  nombre|asignatura|grade|\n",
      "+---+--------+----------+-----+\n",
      "|  4|alumno_4| geometria|    4|\n",
      "|  3|alumno_3|     etica|    4|\n",
      "|  2|alumno_2|    fisica|    4|\n",
      "|  1|alumno_1|   calculo|    4|\n",
      "+---+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ordenamiento compuesto\n",
    "df_notas.sort(\n",
    "    [\"id\", \"grade\"], \n",
    "    ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+-----+\n",
      "| id|  nombre|asignatura|grade|\n",
      "+---+--------+----------+-----+\n",
      "|  1|alumno_1|   calculo|    4|\n",
      "+---+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filtrado de datos\n",
    "df_notas[df_notas[\"grade\"] == 4]\n",
    "\n",
    "#usando filter (una transformacion, forma mas optimizada)\n",
    "df_notas.filter((F.col(\"grade\") == 4) & (F.col(\"id\") == 1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet:\n",
    "- Minimiza memoria.\n",
    "- En la mayorioa de casos el esquema de la data se mantiene.\n",
    "- Lectura mas rapida.\n",
    "- Es el estandar para archivos nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leer csv\n",
    "df_csv = spark.read.csv(\n",
    "    \"C:\\Users\\Admin\\Desktop\\projects\\stores_sales_bi\\recursos\\otros\\uncleaned_datasets\\carsWorldWide.csv\", \n",
    "    header = True,\n",
    "    nullValue= \"?\", #definir string como nulo\n",
    "    sep = \",\", #separador\n",
    "    encoding = \"unicode_escape\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
